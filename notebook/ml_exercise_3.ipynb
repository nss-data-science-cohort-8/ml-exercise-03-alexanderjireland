{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47b33f55",
   "metadata": {},
   "source": [
    "## Machine Learning Exercise 3: Bias and Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f922cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.impute import *\n",
    "from sklearn.model_selection import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eba739",
   "metadata": {},
   "source": [
    "**Bias** refers to the error introduced by approximating a complex real-world problem with a simplified model, while **variance** refers to the model's sensitivity to fluctuations in the training data. A linear regression model has high bias and low variance; it makes strong assumptions about the data (linearity) but is stable across different datasets. If these strong assumptions are not correct, there will be places where it systematically overestimates or underestimates. In contrast, a decision tree model has low bias and high variance;it can capture complex patterns but is prone to overfitting, especially if deep and unpruned. This means that it can start to memorize the training data rather than capturing patterns that generalize.\n",
    "\n",
    "1. Fit a linear regression model to the housing data, using sqft_living to predict price. Check the mean squared error on the training data and the test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d24f5856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error:  74509993356.49603\n"
     ]
    }
   ],
   "source": [
    "housing_df = pd.read_csv(\"../data/kc_house_data.csv\")\n",
    "\n",
    "# specify target and predictor(s)\n",
    "predictors = ['sqft_living']\n",
    "target = 'price'\n",
    "\n",
    "X = housing_df[predictors]\n",
    "y = housing_df[target]\n",
    "\n",
    "# train, test, split housing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)\n",
    "\n",
    "# create Linear Regression model\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "y_pred = linreg.predict(X_test)\n",
    "\n",
    "print(\"mean squared error: \", mean_squared_error(y_test, y_pred))\n",
    "#print(\"root mean squared error: \", root_mean_squared_error(y_test, y_pred))\n",
    "#print(\"mean absolute error: \", mean_absolute_error(y_test, y_pred))\n",
    "#print(\"mean absolute percentage error: \", mean_absolute_percentage_error(y_test, y_pred))\n",
    "#print(\"R^2: \", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccdff0b",
   "metadata": {},
   "source": [
    "2. Repeat this but with a [DecisionTreeRegresor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html). Again check the mean squared error on the training data and the test data. How does what you see differ from the linear regression model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f7c231c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error:  79044952597.92511\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# create DecisionTreeRegressor model\n",
    "dtreereg = DecisionTreeRegressor()\n",
    "dtreereg.fit(X_train, y_train)\n",
    "y_pred = dtreereg.predict(X_test)\n",
    "\n",
    "print(\"mean squared error: \", mean_squared_error(y_test, y_pred))\n",
    "#print(\"root mean squared error: \", root_mean_squared_error(y_test, y_pred))\n",
    "#print(\"mean absolute error: \", mean_absolute_error(y_test, y_pred))\n",
    "#print(\"mean absolute percentage error: \", mean_absolute_percentage_error(y_test, y_pred))\n",
    "#print(\"R^2: \", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4460b2",
   "metadata": {},
   "source": [
    "The Decision Tree Regressor does worse than the linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd39405",
   "metadata": {},
   "source": [
    "One way of avoiding overfitting is by restricting the flexibility of the model. We can do this with a decision tree by restricting the number of splits that it can perform. \n",
    "\n",
    "3. Fit a DecisionTreeRegressor where you restrict the max_depth to 5. Again check the mean squared error on the training data and the test data. What do you notice now?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "47968832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error:  73264250436.88351\n"
     ]
    }
   ],
   "source": [
    "# create DecisionTreeRegressor model with max depth of 5\n",
    "dtreereg = DecisionTreeRegressor(max_depth = 5)\n",
    "dtreereg.fit(X_train, y_train)\n",
    "y_pred = dtreereg.predict(X_test)\n",
    "\n",
    "print(\"mean squared error: \", mean_squared_error(y_test, y_pred))\n",
    "#print(\"root mean squared error: \", root_mean_squared_error(y_test, y_pred))\n",
    "#print(\"mean absolute error: \", mean_absolute_error(y_test, y_pred))\n",
    "#print(\"mean absolute percentage error: \", mean_absolute_percentage_error(y_test, y_pred))\n",
    "#print(\"R^2: \", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2091a892",
   "metadata": {},
   "source": [
    "The mean squared error is much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc532eb",
   "metadata": {},
   "source": [
    "When working with machine learning models, we often have to balance bias and variance. This is called the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). One method of this is through [regularization](https://www.ibm.com/think/topics/regularization), where we restrict the complexity of the model, adding some bias but reducing the variance, which can lead to a lower mean squared error on the test set.\n",
    "\n",
    "Lasso and ridge regression do this by adding a penalty term based on the size of the coefficients. Smaller coefficients means that the model has less flexibility. The neat thing about these types of models is that they determine how to allocate the coefficients automatically as part of the model fitting process, so we can start with a large set of potential predictors and allow the model fitting to determine which ones to focus on.\n",
    "\n",
    "For the next part of the exercise, we'll see how we can add complexity to our model but control the complexity through regularization.\n",
    "\n",
    "4. So far, we've only been predicting based off of the square footage of living space. Fit a new linear regression model using all variables besides id, date, price, and zipcode. How well does this model perform on the test set compared to the model with just square footage of living space?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4b549d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "price",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "bedrooms",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "bathrooms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sqft_living",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sqft_lot",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "floors",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "waterfront",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "view",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "condition",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "grade",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sqft_above",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sqft_basement",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "yr_built",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "yr_renovated",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "zipcode",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "lat",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "long",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sqft_living15",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sqft_lot15",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "4e27109b-5b2f-4408-a5e3-c8c542f78189",
       "rows": [
        [
         "0",
         "7129300520",
         "20141013T000000",
         "221900.0",
         "3",
         "1.0",
         "1180",
         "5650",
         "1.0",
         "0",
         "0",
         "3",
         "7",
         "1180",
         "0",
         "1955",
         "0",
         "98178",
         "47.5112",
         "-122.257",
         "1340",
         "5650"
        ],
        [
         "1",
         "6414100192",
         "20141209T000000",
         "538000.0",
         "3",
         "2.25",
         "2570",
         "7242",
         "2.0",
         "0",
         "0",
         "3",
         "7",
         "2170",
         "400",
         "1951",
         "1991",
         "98125",
         "47.721",
         "-122.319",
         "1690",
         "7639"
        ],
        [
         "2",
         "5631500400",
         "20150225T000000",
         "180000.0",
         "2",
         "1.0",
         "770",
         "10000",
         "1.0",
         "0",
         "0",
         "3",
         "6",
         "770",
         "0",
         "1933",
         "0",
         "98028",
         "47.7379",
         "-122.233",
         "2720",
         "8062"
        ],
        [
         "3",
         "2487200875",
         "20141209T000000",
         "604000.0",
         "4",
         "3.0",
         "1960",
         "5000",
         "1.0",
         "0",
         "0",
         "5",
         "7",
         "1050",
         "910",
         "1965",
         "0",
         "98136",
         "47.5208",
         "-122.393",
         "1360",
         "5000"
        ],
        [
         "4",
         "1954400510",
         "20150218T000000",
         "510000.0",
         "3",
         "2.0",
         "1680",
         "8080",
         "1.0",
         "0",
         "0",
         "3",
         "8",
         "1680",
         "0",
         "1987",
         "0",
         "98074",
         "47.6168",
         "-122.045",
         "1800",
         "7503"
        ]
       ],
       "shape": {
        "columns": 21,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  ...  grade  sqft_above  sqft_basement  \\\n",
       "0      5650     1.0           0     0  ...      7        1180              0   \n",
       "1      7242     2.0           0     0  ...      7        2170            400   \n",
       "2     10000     1.0           0     0  ...      6         770              0   \n",
       "3      5000     1.0           0     0  ...      7        1050            910   \n",
       "4      8080     1.0           0     0  ...      8        1680              0   \n",
       "\n",
       "   yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
       "0      1955             0    98178  47.5112 -122.257           1340   \n",
       "1      1951          1991    98125  47.7210 -122.319           1690   \n",
       "2      1933             0    98028  47.7379 -122.233           2720   \n",
       "3      1965             0    98136  47.5208 -122.393           1360   \n",
       "4      1987             0    98074  47.6168 -122.045           1800   \n",
       "\n",
       "   sqft_lot15  \n",
       "0        5650  \n",
       "1        7639  \n",
       "2        8062  \n",
       "3        5000  \n",
       "4        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df.head(5)\n",
    "\n",
    "#categorical = ['bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'condition', 'grade', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0e51de37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error:  44093475276.573814\n"
     ]
    }
   ],
   "source": [
    "X = housing_df.drop(columns=['id', 'date', 'price', 'zipcode'])\n",
    "y = housing_df['price']\n",
    "\n",
    "# train, test, split housing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)\n",
    "\n",
    "# create Linear Regression model\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "y_pred = linreg.predict(X_test)\n",
    "\n",
    "print(\"mean squared error: \", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b911d",
   "metadata": {},
   "source": [
    "This model performs significantly better than just using the square footage as a predictor!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3ec6b3",
   "metadata": {},
   "source": [
    "5. Try fitting a lasso and ridge model. Becuase lasso and ridge have penalty terms based on the size of the coefficients, and the size of the coefficients depends on the scale of the variable, you'll want to scale the features first so that they are on comparable scales. Create a [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) object where the first step is applying a [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) and the second step is either a lasso or ridge model. Because these models have a hyperparameter controlling regularization strength, you'll want to use the [LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) and [RidgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) models, which will select values for the regularization strength using cross-validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5f8d350e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error:  44128929521.629326\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    steps = [('scaler', StandardScaler()),\n",
    "             ('model', LassoCV())]\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"mean squared error: \", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "09ae0446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error:  44094763932.53586\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    steps = [('scaler', StandardScaler()),\n",
    "             ('model', RidgeCV())]\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"mean squared error: \", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0287a3a1",
   "metadata": {},
   "source": [
    "You likely didn't see much difference between the regular linear regression model and the lasso or ridge model. Let's see what happens if we add more complexity through feature interactions. We can capture more complex relationships between the predictor variables and the target variable by multiplying the predictors together before fitting the model. For example, the interaction between sqft_living and bedrooms will let the model capture if the impact of square footage depends on the number of bedrooms.\n",
    "\n",
    "6. Add [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) to your pipeline after the standard scaler. Try using degree 2 features. How does this change the performance of the regular linear regression model, the lasso model, and the ridge model? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7c2ce3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error:  30926964722.88152\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    steps = [('scaler', StandardScaler()),\n",
    "             ('pf', PolynomialFeatures(degree=2)),\n",
    "             ('model', LinearRegression())]\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"mean squared error: \", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f9bd3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error:  30933833734.684982\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    steps = [('scaler', StandardScaler()),\n",
    "             ('pf', PolynomialFeatures()),\n",
    "             ('model', RidgeCV())]\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"mean squared error: \", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5104a878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error:  30414023904.742867\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    steps = [('scaler', StandardScaler()),\n",
    "             ('pf', PolynomialFeatures(degree=2)),\n",
    "             ('model', LassoCV())]\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"mean squared error: \", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1891b44",
   "metadata": {},
   "source": [
    "The lasso penalty tends to cause some coeffients to zero out, so it can be viewed as a method of automatic feature selection.\n",
    "\n",
    "7. Look at the set of coefficients for the lasso model. What percentage of the coefficients are zero? What are the largest non-zero coefficients?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aea8e1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00 -0.00000000e+00  1.29631038e+04  9.30898037e+04\n",
      " -0.00000000e+00 -7.40640157e+03  0.00000000e+00  0.00000000e+00\n",
      "  2.55901284e+04  9.49584662e+04  3.48881548e+03  0.00000000e+00\n",
      " -1.89758098e+04  0.00000000e+00  6.91512974e+04 -2.23709363e+04\n",
      "  4.64911392e+04 -0.00000000e+00  1.34675819e+02 -3.42301923e+03\n",
      " -5.54586218e+03  0.00000000e+00  0.00000000e+00 -8.21509899e+02\n",
      " -0.00000000e+00  2.68731640e+02  4.54886920e+02 -0.00000000e+00\n",
      " -7.66847183e+02 -4.16932354e+02 -2.08144768e+03 -0.00000000e+00\n",
      "  5.53855599e+03  2.71218569e+02  1.37786566e+03 -0.00000000e+00\n",
      "  1.94692202e+04 -3.77436715e+03 -5.05144532e+03 -0.00000000e+00\n",
      "  2.70220671e+03 -1.27452732e+03  9.42420011e+02  7.47394194e+03\n",
      "  0.00000000e+00  3.62574286e+03 -4.04970904e+03  0.00000000e+00\n",
      " -1.84220237e+03 -6.94222056e+02  0.00000000e+00  4.36724451e+03\n",
      " -5.07516407e+03 -0.00000000e+00  6.50325965e+03  0.00000000e+00\n",
      "  0.00000000e+00  3.34707747e+04  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.12849069e+04  1.88427149e+04 -1.92974825e+04\n",
      "  0.00000000e+00 -0.00000000e+00 -3.64151338e+00  0.00000000e+00\n",
      "  1.05481548e+03  1.64917083e+03  0.00000000e+00  1.82774005e+03\n",
      " -0.00000000e+00 -0.00000000e+00  7.50998142e+03 -3.01402238e+03\n",
      " -2.77950863e+03  4.86254680e+03  0.00000000e+00  1.38982217e+03\n",
      "  0.00000000e+00 -7.17748731e+03 -0.00000000e+00  4.24076570e+03\n",
      " -0.00000000e+00 -4.13340615e+03 -0.00000000e+00  0.00000000e+00\n",
      " -5.41410094e+02  0.00000000e+00 -0.00000000e+00 -2.19323615e+03\n",
      "  0.00000000e+00  4.44911272e+03 -1.24430690e+03  1.43351210e+03\n",
      " -5.90085930e+03  8.01702457e+03  0.00000000e+00  7.39133793e+03\n",
      "  1.13592527e+03  1.03483293e+04  1.11287126e+04  3.45395611e+03\n",
      "  4.78931439e+02  5.52222147e+03  1.90564838e+03  5.82835092e+03\n",
      "  0.00000000e+00 -1.16391318e+03  6.69986100e+02 -4.88595101e+03\n",
      "  1.21118164e+03  8.46343571e+02 -0.00000000e+00 -1.65503550e+03\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00  8.91210628e+02\n",
      " -1.84027167e+03 -3.31997947e+03 -9.40398072e+02  0.00000000e+00\n",
      "  1.32212070e+04 -5.26397475e+03 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  2.95522228e+03  1.95939812e+04\n",
      " -2.16187768e+04 -1.74047561e+02 -6.38990989e+02  0.00000000e+00\n",
      "  6.84237785e+02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -3.12891438e+03 -0.00000000e+00 -0.00000000e+00 -1.32151366e+04\n",
      " -2.44709272e+03  2.22387716e+03  0.00000000e+00 -0.00000000e+00\n",
      "  7.17101323e+03 -0.00000000e+00  1.98399910e+04  1.34178964e+03\n",
      " -1.16413417e+04  1.38986201e+04 -3.27771757e+04  2.45260297e+03\n",
      "  3.34830745e+03 -1.60201027e+03  3.56115240e+03  6.44427138e+03\n",
      " -0.00000000e+00 -4.10425611e+04 -1.45047685e+04  2.87635097e+03\n",
      " -9.03345392e+03 -2.67713957e+03 -2.98477385e+03  3.72643823e+02\n",
      "  3.59010509e+03 -4.60464565e+03 -7.57699582e+02]\n",
      "Percent nonzero coefficients:  0.6491228070175439\n",
      "Percent zero coefficients:  0.3508771929824561\n",
      "94958.4661777975\n"
     ]
    }
   ],
   "source": [
    "lasso_model = pipeline.named_steps['model']\n",
    "coefficients = lasso_model.coef_\n",
    "\n",
    "print(coefficients)\n",
    "print(\"Percent nonzero coefficients: \", np.count_nonzero(coefficients)/len(coefficients))\n",
    "print(\"Percent zero coefficients: \", 1 - (np.count_nonzero(coefficients)/len(coefficients)))\n",
    "print(max(coefficients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "491dbca3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lasso'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\irela\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:316\u001b[0m, in \u001b[0;36mPipeline.__getitem__\u001b[1;34m(self, ind)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 316\u001b[0m     name, est \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[ind]\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;66;03m# Not an int, try get step by name\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m coefficients \u001b[38;5;241m=\u001b[39m lasso_model\u001b[38;5;241m.\u001b[39mcoef_\n\u001b[0;32m      2\u001b[0m coefficients \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariable\u001b[39m\u001b[38;5;124m'\u001b[39m: pipeline[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mget_feature_names_out(),\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoefficient\u001b[39m\u001b[38;5;124m'\u001b[39m: pipeline[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlasso\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcoef_\n\u001b[0;32m      5\u001b[0m })\n",
      "File \u001b[1;32mc:\\Users\\irela\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:319\u001b[0m, in \u001b[0;36mPipeline.__getitem__\u001b[1;34m(self, ind)\u001b[0m\n\u001b[0;32m    316\u001b[0m     name, est \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[ind]\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;66;03m# Not an int, try get step by name\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_steps[ind]\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m est\n",
      "File \u001b[1;32mc:\\Users\\irela\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_bunch.py:39\u001b[0m, in \u001b[0;36mBunch.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_deprecated_key_to_warnings\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}):\n\u001b[0;32m     35\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deprecated_key_to_warnings[key],\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m     38\u001b[0m     )\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'lasso'"
     ]
    }
   ],
   "source": [
    "coefficients = lasso_model.coef_\n",
    "coefficients = pd.DataFrame({\n",
    "    'variable': pipeline[:-1].get_feature_names_out(),\n",
    "    'coefficient': pipeline['lasso'].coef_\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1892a5bb",
   "metadata": {},
   "source": [
    "8. A new hyperparameter that we have is the degree of the polynomial we're using. So that we're not overfitting to the test set, we need to use cross-validation to select this value. Set up a [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to try out polynomial degrees from 1 to 3 and to try LinearRegression, LassoCV, and RidgeCV models. Use 'neg_mean_squared_error' as the error_score. Which combination does it find does the best? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23021749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pf__degree': 2}\n",
      "mean squared error:  30926964722.88152\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    steps = [('scaler', StandardScaler()),\n",
    "             ('pf', PolynomialFeatures()),\n",
    "             ('model', LinearRegression())]\n",
    ")\n",
    "\n",
    "cv_pipe = GridSearchCV(pipeline,\n",
    "                       param_grid={'pf__degree': [1,2,3]},\n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       n_jobs = -1,\n",
    "                       cv = 5)\n",
    "\n",
    "cv_pipe.fit(X_train, y_train)\n",
    "print(cv_pipe.best_params_)\n",
    "y_pred = cv_pipe.predict(X_test)\n",
    "\n",
    "print(\"mean squared error: \", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f567fc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pf__degree': 2}\n",
      "mean squared error:  30933833734.684982\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    steps = [('scaler', StandardScaler()),\n",
    "             ('pf', PolynomialFeatures()),\n",
    "             ('model', RidgeCV())]\n",
    ")\n",
    "\n",
    "cv_pipe = GridSearchCV(pipeline,\n",
    "                       param_grid={'pf__degree': [1,2,3]},\n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       n_jobs = -1,\n",
    "                       cv = 5)\n",
    "\n",
    "cv_pipe.fit(X_train, y_train)\n",
    "print(cv_pipe.best_params_)\n",
    "y_pred = cv_pipe.predict(X_test)\n",
    "\n",
    "print(\"mean squared error: \", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9724915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pf__degree': 2}\n",
      "mean squared error:  30414023904.742867\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    steps = [('scaler', StandardScaler()),\n",
    "             ('pf', PolynomialFeatures()),\n",
    "             ('model', LassoCV())]\n",
    ")\n",
    "\n",
    "cv_pipe = GridSearchCV(pipeline,\n",
    "                       param_grid={'pf__degree': [1,2,3]},\n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       n_jobs = -1,\n",
    "                       cv = 5)\n",
    "\n",
    "cv_pipe.fit(X_train, y_train)\n",
    "print(cv_pipe.best_params_)\n",
    "y_pred = cv_pipe.predict(X_test)\n",
    "\n",
    "print(\"mean squared error: \", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0ee67b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score for degree=1, model=LinearRegression(): 0.6946\n",
      "Test score for degree=1, model=LassoCV(max_iter=10000): 0.6943\n",
      "Test score for degree=1, model=RidgeCV(): 0.6946\n",
      "Test score for degree=2, model=LinearRegression(): 0.7858\n",
      "Test score for degree=2, model=LassoCV(max_iter=10000): 0.7893\n",
      "Test score for degree=2, model=RidgeCV(): 0.7857\n",
      "Test score for degree=3, model=LinearRegression(): -2535533486043997.5000\n",
      "Test score for degree=3, model=LassoCV(max_iter=10000): 0.7711\n",
      "Test score for degree=3, model=RidgeCV(): -0.2823\n",
      "\n",
      "Best parameters: {'degree': 2, 'model': LassoCV(max_iter=10000)}\n",
      "Best score: 0.7893277370558397\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "degrees = [1, 2, 3]\n",
    "models = [LinearRegression(), LassoCV(max_iter = 10000), RidgeCV()]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "best_score = float(\"-inf\")\n",
    "best_params = None\n",
    "for degree, model in product(degrees, models):\n",
    "    pipeline = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"poly\", PolynomialFeatures(degree=degree)),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    score = pipeline.score(X_test, y_test)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = {\"degree\": degree, \"model\": model}\n",
    "    print(f\"Test score for degree={degree}, model={model}: {score:.4f}\")\n",
    "print(\"\\nBest parameters:\", best_params)\n",
    "print(\"Best score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8a4397eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Best parameters: {'model': LinearRegression(), 'poly__degree': 2}\n",
      "Best model: Pipeline(steps=[('scaler', StandardScaler()), ('poly', PolynomialFeatures()),\n",
      "                ('model', LinearRegression())])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),    # Standardize features\n",
    "    (\"poly\", PolynomialFeatures()),  # Placeholder for polynomial transformation\n",
    "    (\"model\", LinearRegression())    # Placeholder for model\n",
    "])\n",
    "param_grid = {\n",
    "    \"poly__degree\": [1, 2, 3],  # Different polynomial degrees\n",
    "    \"model\": [LinearRegression(), LassoCV(max_iter = 10000), RidgeCV()],  # Different models\n",
    "}\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1, verbose = 2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best model:\", grid_search.best_estimator_)\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a64183",
   "metadata": {},
   "source": [
    "** If you've reached this point, let your instructors know so that they can check in with you. **\n",
    "\n",
    "Stretch Goals:\n",
    "\n",
    "1. With home prices, there are some extremely large values for price, which can overly-influence the mean squared error calculation. See what happens if you optimize for mean absolute error instead. Alternatively, try using a [TransformedTargetRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.compose.TransformedTargetRegressor.html) to predict the log of price instead of the raw price.\n",
    "\n",
    "**Bonus Exercise** We've seen how a decision tree model has move flexibility, which mean higher variance compared to a linear regression model. One way of understanding variance is that variance describes how sensitive the model is to the training data. A model with high variance can produce vastly different predictions when trained on different subsets.\n",
    "\n",
    "In this bonus exercise, you'll get to see this in action.\n",
    "\n",
    "Generate 1000 different linear regression fits, where you only use sqft_living as the predictor variable. For each fit, choose a random sample from the full dataset of size 1000. \n",
    "Using the np.linspace function, generate a grid of 100 equally-spaced points between 500 and 3000 and generate predictions on those points. Plot the mean prediction, the 5th percentile, and the 95th percentile for the predictions from these thousand models. Repeat this for a decision tree model. Then do it for a decision tree model with a max_depth of 5.\n",
    "\n",
    "How do these predictions differ? Which have the most variability?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
